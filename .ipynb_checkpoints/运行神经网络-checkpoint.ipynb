{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals,print_function,division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "device =torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Summary:\n",
    "    将音乐转换为note\n",
    "Parameters:\n",
    "    data - libsora读取到的y\n",
    "    rate - libsora读取到的rate\n",
    "Return:\n",
    "    Note - note列表\n",
    "'''\n",
    "def music2note(data,rate):\n",
    "    #转置 降噪\n",
    "    data=np.transpose(data)\n",
    "    data=data/data.max()\n",
    "    data[data<0.75]=0\n",
    "\n",
    "    #单位频率\n",
    "    unit_rate=rate/(data.shape[1]-1)\n",
    "    Note=[]\n",
    "\n",
    "    for i in data:\n",
    "        note=[]\n",
    "        for j in range(len(i)):\n",
    "            #第一个非0值\n",
    "            if i[j]>0 and len(note)==0:\n",
    "                #频率转换音符\n",
    "                note_str = librosa.hz_to_note((j+1)*unit_rate)\n",
    "                #print(note_str)\n",
    "                #if 'Db' == note_str[:2]\n",
    "                if note_str[1] == '#':\n",
    "                    note_str = note_str[0]+note_str[2]+'#'\n",
    "                note.append(note_str)\n",
    "                \n",
    "        if len(note)!=0:\n",
    "            Note.append(note[0])\n",
    "    return  Note\n",
    "\n",
    "'''\n",
    "Summary:\n",
    "    调用该函数返回最终的note列表\n",
    "\n",
    "Return:\n",
    "    data - note列表\n",
    "'''\n",
    "def ChangeMuisc2Note(filpath):\n",
    "    y,rate=librosa.load(filpath,44100)\n",
    "    #短时傅里叶\n",
    "    fft=librosa.stft(y,n_fft=1024*2)\n",
    "    #转换为分贝值\n",
    "    D=librosa.amplitude_to_db(abs(fft),ref=np.max)\n",
    "    #数据>0\n",
    "    D=D+80\n",
    "    \n",
    "    data=music2note(D,rate/2)\n",
    "\n",
    "#     output=open('note.txt','w+')\n",
    "#     for i in range(len(data)):\n",
    "#         output.write(data[i])\n",
    "#         output.write('\\n')\n",
    "#     output.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def getData():\n",
    "    path='wavDatasets//'\n",
    "    listdir=os.listdir(path)\n",
    "    data={}\n",
    "    for x in listdir:\n",
    "        if 'wav' in x:\n",
    "            y = ChangeMuisc2Note('wavDatasets//'+x)\n",
    "            y = one_hot(y)\n",
    "            #print(y)\n",
    "            data[x[0:-4]]=y\n",
    "    return data\n",
    "\n",
    "def getLabel():\n",
    "    df=open('wavDatasets//music_dic.pkl','rb')#注意此处是rb\n",
    "    #此处使用的是load(目标文件)\n",
    "    music_dic=pickle.load(df)\n",
    "    df.close()\n",
    "    label={}\n",
    "    for x in music_dic.keys():\n",
    "        y=music_dic[x]\n",
    "        label[x]=one_hot(y)\n",
    "    return label\n",
    "\n",
    "note_label_list = ['<S>','<E>','other','C4','C#4','D4','D#4','E4','F4','F#4',\n",
    "                   'G4','G#4','A4','A#4','B4','C5','C#5','D5','D#5','E5','F5',\n",
    "                   'F#5','G5','G#5','A5','A#5','B5','C6','C#6','D6','D#6','E6',\n",
    "                   'F6','F#6','G6','G#6','A6','A#6','B6','C7','C#7']\n",
    "\n",
    "#得到note和num字典\n",
    "def getDict():\n",
    "    note_dic = {}\n",
    "    for x in range(len(note_label_list)):\n",
    "        if note_label_list[x] not in note_dic.keys():\n",
    "            # onehot = np.zeros(len(note))\n",
    "            # onehot[x] = 1\n",
    "            note_dic[note_label_list[x]] = x\n",
    "    num_dic=dict(zip(note_dic.values(),note_dic.keys()))\n",
    "\n",
    "    return note_dic,num_dic\n",
    "\n",
    "note_dic,num_dic = getDict()\n",
    "\n",
    "def one_hot(y):\n",
    "    note=[]\n",
    "    note.append(note_dic['<S>'])\n",
    "    for x in y:\n",
    "        try:\n",
    "            note.append(note_dic[x])\n",
    "        except:\n",
    "            note.append(note_dic['other'])\n",
    "    note.append(note_dic['<E>'])\n",
    "    return note\n",
    "\n",
    "#会调用前面的函数，最后返回训练集的数据\n",
    "#形状为（x_train,y_train)，x_train是训练集音频数据，y_train是训练集数据标签\n",
    "#数据类型均为torch.longtensor\n",
    "def getPair():\n",
    "    x=getData()\n",
    "    #print(x)\n",
    "    y=getLabel()\n",
    "    data=[]\n",
    "    label=[]\n",
    "    for key in x.keys():\n",
    "        a=x[key]\n",
    "        a=list(a)\n",
    "        #print(a)\n",
    "        a=torch.Tensor(a).view(len(a),1).long()\n",
    "        b=torch.Tensor(y[key]).view(len(y[key]),1).long()\n",
    "        a = a.to(device)\n",
    "        #print(a)\n",
    "        b = b.to(device)\n",
    "        data.append(a)\n",
    "        label.append(b)\n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n"
     ]
    }
   ],
   "source": [
    "#获取数据和字典\n",
    "x,y = getPair()\n",
    "MAX_LENGTH = max([i.shape[0] for i in x])\n",
    "S_token=0#代表句子的开始\n",
    "E_token=1#代表句子的结束\n",
    "print(len(x),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([685, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x[-1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#保存训练数据\n",
    "file=open('x.pkl','wb')\n",
    "pickle.dump(x,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存标签数据\n",
    "file=open('y.pkl','wb')\n",
    "pickle.dump(y,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取训练数据和标签数据\n",
    "file=open('x.pkl','rb')\n",
    "x=pickle.load(file)\n",
    "file.close()\n",
    "file=open('y.pkl','rb')\n",
    "y=pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "MAX_LENGTH = max([i.shape[0] for i in x])\n",
    "S_token=0#代表句子的开始\n",
    "E_token=1#代表句子的结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分开训练数据和测试数据\n",
    "x_train=x[0:100]\n",
    "y_train=y[0:100]\n",
    "x_test=x[100:-1]\n",
    "y_test=y[100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([911, 1])\n",
      "1289\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0].size())\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不要动它\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "#不要动它\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length=max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat([embedded[0], hidden[0]], 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat([embedded[0], attn_applied[0]], 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "#不要动它\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,maxlen=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(maxlen, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        \n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    decoder_input = torch.tensor([[S_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    teacher_forcing_ratio=0.5\n",
    "    use_teacher_forcing = False#True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the targer as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Without teaching forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == E_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length,encoder,decoder\n",
    "\n",
    "#不要动它\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "#不要动它\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs是训练迭代次数\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1, plot_every=1,save_every=1, learning_rate=0.01,epochs=10):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    epochs=epochs+1\n",
    "    for epoch in range(1,epochs):\n",
    "        print('epoch:',epoch)\n",
    "        for iter in range(1, n_iters + 1):\n",
    "            print('iter:',iter)\n",
    "            input_tensor = x_train[iter-1]\n",
    "            target_tensor = y_train[iter-1]\n",
    "            print('input_tensor:',input_tensor.size())\n",
    "            print('target_tensor:',target_tensor.size())\n",
    "\n",
    "            loss,encoder,decoder= train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "            if iter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                             iter, iter / n_iters * 100, print_loss_avg))\n",
    "                                       \n",
    "\n",
    "            if iter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        if epoch % save_every==0:\n",
    "            torch.save(encoder,'model/ecnoder{}.pkl'.format(str(epoch)))\n",
    "            torch.save(decoder,'model/attn_decoder{}.pkl'.format(str(epoch)))\n",
    "            loss=np.array(plot_losses)\n",
    "            np.save('Loss/loss{}.npy'.format(str(epoch)),loss)\n",
    "                \n",
    "    showPlot(plot_losses)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#不要动它\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "#不要动它\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[S_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "#             print(topi.item())\n",
    "            if topi.item() == E_token:\n",
    "#                 print('<E>')\n",
    "                decoded_words.append('<E>')\n",
    "                break\n",
    "            else:\n",
    "                # print(di,num_dic[topi.item()])\n",
    "                decoded_words.append(num_dic[topi.item()])\n",
    "\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "#x_test是测试音频数据\n",
    "#y_test是测试数据标签\n",
    "#n是测试数据的数量\n",
    "def evaluateRandomly(encoder, decoder, x_test,y_test=None,n=1):\n",
    "    for i in range(n):\n",
    "        print('>', x_test[i].size())\n",
    "        print('=', y_test[i].size())\n",
    "        output_words, attentions = evaluate(encoder, decoder, x_test[i])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "dic_size = len(note_label_list)\n",
    "\n",
    "# 第一个参数是字典大小\n",
    "encoder1 = EncoderRNN(dic_size, hidden_size).to(device)\n",
    "#第二个参数是字典大小\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, dic_size, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(dic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(41, 64)\n",
       "  (gru): GRU(64, 64)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(41, 64)\n",
       "  (attn): Linear(in_features=128, out_features=1289, bias=True)\n",
       "  (attn_combine): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gru): GRU(64, 64)\n",
       "  (out): Linear(in_features=64, out_features=41, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "iter: 1\n",
      "input_tensor: torch.Size([432, 1])\n",
      "target_tensor: torch.Size([12, 1])\n",
      "0m 1s (- 2m 27s) (1 1%) 3.6840\n",
      "iter: 2\n",
      "input_tensor: torch.Size([1200, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 2s (- 1m 52s) (2 2%) 3.6683\n",
      "iter: 3\n",
      "input_tensor: torch.Size([1037, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 2s (- 1m 36s) (3 3%) 3.5844\n",
      "iter: 4\n",
      "input_tensor: torch.Size([1203, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 3s (- 1m 30s) (4 4%) 3.5035\n",
      "iter: 5\n",
      "input_tensor: torch.Size([524, 1])\n",
      "target_tensor: torch.Size([14, 1])\n",
      "0m 4s (- 1m 18s) (5 5%) 3.5156\n",
      "iter: 6\n",
      "input_tensor: torch.Size([693, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 4s (- 1m 11s) (6 6%) 3.3999\n",
      "iter: 7\n",
      "input_tensor: torch.Size([692, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 5s (- 1m 7s) (7 7%) 3.2863\n",
      "iter: 8\n",
      "input_tensor: torch.Size([828, 1])\n",
      "target_tensor: torch.Size([21, 1])\n",
      "0m 5s (- 1m 4s) (8 8%) 3.1984\n",
      "iter: 9\n",
      "input_tensor: torch.Size([689, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 6s (- 1m 1s) (9 9%) 3.4165\n",
      "iter: 10\n",
      "input_tensor: torch.Size([1244, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 6s (- 1m 2s) (10 10%) 3.2968\n",
      "iter: 11\n",
      "input_tensor: torch.Size([561, 1])\n",
      "target_tensor: torch.Size([15, 1])\n",
      "0m 7s (- 0m 59s) (11 11%) 3.1059\n",
      "iter: 12\n",
      "input_tensor: torch.Size([597, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 7s (- 0m 56s) (12 12%) 3.4813\n",
      "iter: 13\n",
      "input_tensor: torch.Size([1070, 1])\n",
      "target_tensor: torch.Size([27, 1])\n",
      "0m 8s (- 0m 56s) (13 13%) 3.3162\n",
      "iter: 14\n",
      "input_tensor: torch.Size([1279, 1])\n",
      "target_tensor: torch.Size([32, 1])\n",
      "0m 9s (- 0m 57s) (14 14%) 3.1933\n",
      "iter: 15\n",
      "input_tensor: torch.Size([781, 1])\n",
      "target_tensor: torch.Size([20, 1])\n",
      "0m 9s (- 0m 55s) (15 15%) 3.3899\n",
      "iter: 16\n",
      "input_tensor: torch.Size([864, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 10s (- 0m 54s) (16 16%) 3.2284\n",
      "iter: 17\n",
      "input_tensor: torch.Size([475, 1])\n",
      "target_tensor: torch.Size([13, 1])\n",
      "0m 10s (- 0m 52s) (17 17%) 3.3067\n",
      "iter: 18\n",
      "input_tensor: torch.Size([1252, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 11s (- 0m 52s) (18 18%) 3.1723\n",
      "iter: 19\n",
      "input_tensor: torch.Size([980, 1])\n",
      "target_tensor: torch.Size([25, 1])\n",
      "0m 12s (- 0m 51s) (19 19%) 3.3094\n",
      "iter: 20\n",
      "input_tensor: torch.Size([1242, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 12s (- 0m 51s) (20 20%) 3.1540\n",
      "iter: 21\n",
      "input_tensor: torch.Size([1207, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 13s (- 0m 51s) (21 21%) 2.9707\n",
      "iter: 22\n",
      "input_tensor: torch.Size([1216, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 14s (- 0m 51s) (22 22%) 3.1371\n",
      "iter: 23\n",
      "input_tensor: torch.Size([949, 1])\n",
      "target_tensor: torch.Size([24, 1])\n",
      "0m 15s (- 0m 51s) (23 23%) 2.8758\n",
      "iter: 24\n",
      "input_tensor: torch.Size([1244, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 16s (- 0m 51s) (24 24%) 3.0785\n",
      "iter: 25\n",
      "input_tensor: torch.Size([606, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 16s (- 0m 49s) (25 25%) 3.2898\n",
      "iter: 26\n",
      "input_tensor: torch.Size([1122, 1])\n",
      "target_tensor: torch.Size([28, 1])\n",
      "0m 17s (- 0m 49s) (26 26%) 3.0296\n",
      "iter: 27\n",
      "input_tensor: torch.Size([523, 1])\n",
      "target_tensor: torch.Size([14, 1])\n",
      "0m 17s (- 0m 47s) (27 27%) 3.1035\n",
      "iter: 28\n",
      "input_tensor: torch.Size([1023, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 18s (- 0m 47s) (28 28%) 2.8359\n",
      "iter: 29\n",
      "input_tensor: torch.Size([1035, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 19s (- 0m 46s) (29 28%) 2.9430\n",
      "iter: 30\n",
      "input_tensor: torch.Size([901, 1])\n",
      "target_tensor: torch.Size([23, 1])\n",
      "0m 19s (- 0m 45s) (30 30%) 3.0794\n",
      "iter: 31\n",
      "input_tensor: torch.Size([1245, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 20s (- 0m 45s) (31 31%) 2.9896\n",
      "iter: 32\n",
      "input_tensor: torch.Size([953, 1])\n",
      "target_tensor: torch.Size([24, 1])\n",
      "0m 21s (- 0m 45s) (32 32%) 2.9385\n",
      "iter: 33\n",
      "input_tensor: torch.Size([954, 1])\n",
      "target_tensor: torch.Size([24, 1])\n",
      "0m 21s (- 0m 44s) (33 33%) 2.9385\n",
      "iter: 34\n",
      "input_tensor: torch.Size([1122, 1])\n",
      "target_tensor: torch.Size([28, 1])\n",
      "0m 22s (- 0m 44s) (34 34%) 2.8517\n",
      "iter: 35\n",
      "input_tensor: torch.Size([652, 1])\n",
      "target_tensor: torch.Size([17, 1])\n",
      "0m 23s (- 0m 42s) (35 35%) 2.9430\n",
      "iter: 36\n",
      "input_tensor: torch.Size([519, 1])\n",
      "target_tensor: torch.Size([14, 1])\n",
      "0m 23s (- 0m 41s) (36 36%) 2.9076\n",
      "iter: 37\n",
      "input_tensor: torch.Size([997, 1])\n",
      "target_tensor: torch.Size([25, 1])\n",
      "0m 24s (- 0m 41s) (37 37%) 2.9397\n",
      "iter: 38\n",
      "input_tensor: torch.Size([602, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 24s (- 0m 40s) (38 38%) 3.4007\n",
      "iter: 39\n",
      "input_tensor: torch.Size([1286, 1])\n",
      "target_tensor: torch.Size([32, 1])\n",
      "0m 25s (- 0m 39s) (39 39%) 3.1662\n",
      "iter: 40\n",
      "input_tensor: torch.Size([863, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 26s (- 0m 39s) (40 40%) 2.9530\n",
      "iter: 41\n",
      "input_tensor: torch.Size([773, 1])\n",
      "target_tensor: torch.Size([20, 1])\n",
      "0m 26s (- 0m 38s) (41 41%) 3.1568\n",
      "iter: 42\n",
      "input_tensor: torch.Size([1026, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 27s (- 0m 37s) (42 42%) 3.0330\n",
      "iter: 43\n",
      "input_tensor: torch.Size([1033, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 28s (- 0m 37s) (43 43%) 3.0560\n",
      "iter: 44\n",
      "input_tensor: torch.Size([605, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 28s (- 0m 36s) (44 44%) 3.0444\n",
      "iter: 45\n",
      "input_tensor: torch.Size([1028, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 29s (- 0m 35s) (45 45%) 2.9855\n",
      "iter: 46\n",
      "input_tensor: torch.Size([691, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 29s (- 0m 34s) (46 46%) 2.9090\n",
      "iter: 47\n",
      "input_tensor: torch.Size([782, 1])\n",
      "target_tensor: torch.Size([20, 1])\n",
      "0m 30s (- 0m 33s) (47 47%) 2.8426\n",
      "iter: 48\n",
      "input_tensor: torch.Size([559, 1])\n",
      "target_tensor: torch.Size([15, 1])\n",
      "0m 30s (- 0m 32s) (48 48%) 3.1524\n",
      "iter: 49\n",
      "input_tensor: torch.Size([863, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 31s (- 0m 32s) (49 49%) 2.9911\n",
      "iter: 50\n",
      "input_tensor: torch.Size([1248, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 31s (- 0m 31s) (50 50%) 3.1846\n",
      "iter: 51\n",
      "input_tensor: torch.Size([566, 1])\n",
      "target_tensor: torch.Size([15, 1])\n",
      "0m 32s (- 0m 30s) (51 51%) 2.9582\n",
      "iter: 52\n",
      "input_tensor: torch.Size([1161, 1])\n",
      "target_tensor: torch.Size([29, 1])\n",
      "0m 32s (- 0m 30s) (52 52%) 2.9567\n",
      "iter: 53\n",
      "input_tensor: torch.Size([734, 1])\n",
      "target_tensor: torch.Size([19, 1])\n",
      "0m 33s (- 0m 29s) (53 53%) 2.7152\n",
      "iter: 54\n",
      "input_tensor: torch.Size([978, 1])\n",
      "target_tensor: torch.Size([25, 1])\n",
      "0m 34s (- 0m 29s) (54 54%) 3.1973\n",
      "iter: 55\n",
      "input_tensor: torch.Size([613, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 34s (- 0m 28s) (55 55%) 2.8719\n",
      "iter: 56\n",
      "input_tensor: torch.Size([1115, 1])\n",
      "target_tensor: torch.Size([28, 1])\n",
      "0m 35s (- 0m 27s) (56 56%) 2.8914\n",
      "iter: 57\n",
      "input_tensor: torch.Size([866, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 35s (- 0m 27s) (57 56%) 2.7645\n",
      "iter: 58\n",
      "input_tensor: torch.Size([1032, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 36s (- 0m 26s) (58 57%) 2.8286\n",
      "iter: 59\n",
      "input_tensor: torch.Size([1208, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 37s (- 0m 25s) (59 59%) 2.7783\n",
      "iter: 60\n",
      "input_tensor: torch.Size([433, 1])\n",
      "target_tensor: torch.Size([12, 1])\n",
      "0m 37s (- 0m 25s) (60 60%) 2.6621\n",
      "iter: 61\n",
      "input_tensor: torch.Size([698, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 38s (- 0m 24s) (61 61%) 2.6534\n",
      "iter: 62\n",
      "input_tensor: torch.Size([865, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 38s (- 0m 23s) (62 62%) 2.9510\n",
      "iter: 63\n",
      "input_tensor: torch.Size([816, 1])\n",
      "target_tensor: torch.Size([21, 1])\n",
      "0m 39s (- 0m 23s) (63 63%) 3.1007\n",
      "iter: 64\n",
      "input_tensor: torch.Size([1122, 1])\n",
      "target_tensor: torch.Size([28, 1])\n",
      "0m 40s (- 0m 22s) (64 64%) 2.6094\n",
      "iter: 65\n",
      "input_tensor: torch.Size([611, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 40s (- 0m 21s) (65 65%) 2.2738\n",
      "iter: 66\n",
      "input_tensor: torch.Size([1250, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 41s (- 0m 21s) (66 66%) 2.9138\n",
      "iter: 67\n",
      "input_tensor: torch.Size([1149, 1])\n",
      "target_tensor: torch.Size([29, 1])\n",
      "0m 42s (- 0m 20s) (67 67%) 3.0736\n",
      "iter: 68\n",
      "input_tensor: torch.Size([988, 1])\n",
      "target_tensor: torch.Size([25, 1])\n",
      "0m 42s (- 0m 20s) (68 68%) 3.0183\n",
      "iter: 69\n",
      "input_tensor: torch.Size([731, 1])\n",
      "target_tensor: torch.Size([19, 1])\n",
      "0m 43s (- 0m 19s) (69 69%) 2.8983\n",
      "iter: 70\n",
      "input_tensor: torch.Size([422, 1])\n",
      "target_tensor: torch.Size([12, 1])\n",
      "0m 43s (- 0m 18s) (70 70%) 3.2445\n",
      "iter: 71\n",
      "input_tensor: torch.Size([910, 1])\n",
      "target_tensor: torch.Size([23, 1])\n",
      "0m 44s (- 0m 17s) (71 71%) 2.7477\n",
      "iter: 72\n",
      "input_tensor: torch.Size([1042, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 44s (- 0m 17s) (72 72%) 2.7740\n",
      "iter: 73\n",
      "input_tensor: torch.Size([823, 1])\n",
      "target_tensor: torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 45s (- 0m 16s) (73 73%) 2.7628\n",
      "iter: 74\n",
      "input_tensor: torch.Size([1289, 1])\n",
      "target_tensor: torch.Size([32, 1])\n",
      "0m 46s (- 0m 16s) (74 74%) 2.6587\n",
      "iter: 75\n",
      "input_tensor: torch.Size([1080, 1])\n",
      "target_tensor: torch.Size([27, 1])\n",
      "0m 46s (- 0m 15s) (75 75%) 3.0197\n",
      "iter: 76\n",
      "input_tensor: torch.Size([908, 1])\n",
      "target_tensor: torch.Size([23, 1])\n",
      "0m 47s (- 0m 14s) (76 76%) 2.8506\n",
      "iter: 77\n",
      "input_tensor: torch.Size([1077, 1])\n",
      "target_tensor: torch.Size([27, 1])\n",
      "0m 48s (- 0m 14s) (77 77%) 2.6494\n",
      "iter: 78\n",
      "input_tensor: torch.Size([566, 1])\n",
      "target_tensor: torch.Size([15, 1])\n",
      "0m 48s (- 0m 13s) (78 78%) 2.6180\n",
      "iter: 79\n",
      "input_tensor: torch.Size([1160, 1])\n",
      "target_tensor: torch.Size([29, 1])\n",
      "0m 49s (- 0m 13s) (79 79%) 3.0549\n",
      "iter: 80\n",
      "input_tensor: torch.Size([479, 1])\n",
      "target_tensor: torch.Size([13, 1])\n",
      "0m 49s (- 0m 12s) (80 80%) 2.8028\n",
      "iter: 81\n",
      "input_tensor: torch.Size([1261, 1])\n",
      "target_tensor: torch.Size([31, 1])\n",
      "0m 50s (- 0m 11s) (81 81%) 2.5426\n",
      "iter: 82\n",
      "input_tensor: torch.Size([480, 1])\n",
      "target_tensor: torch.Size([13, 1])\n",
      "0m 50s (- 0m 11s) (82 82%) 3.0340\n",
      "iter: 83\n",
      "input_tensor: torch.Size([474, 1])\n",
      "target_tensor: torch.Size([13, 1])\n",
      "0m 51s (- 0m 10s) (83 83%) 3.0456\n",
      "iter: 84\n",
      "input_tensor: torch.Size([811, 1])\n",
      "target_tensor: torch.Size([21, 1])\n",
      "0m 51s (- 0m 9s) (84 84%) 2.8344\n",
      "iter: 85\n",
      "input_tensor: torch.Size([442, 1])\n",
      "target_tensor: torch.Size([12, 1])\n",
      "0m 51s (- 0m 9s) (85 85%) 2.3426\n",
      "iter: 86\n",
      "input_tensor: torch.Size([519, 1])\n",
      "target_tensor: torch.Size([14, 1])\n",
      "0m 52s (- 0m 8s) (86 86%) 2.8674\n",
      "iter: 87\n",
      "input_tensor: torch.Size([870, 1])\n",
      "target_tensor: torch.Size([22, 1])\n",
      "0m 52s (- 0m 7s) (87 87%) 2.7137\n",
      "iter: 88\n",
      "input_tensor: torch.Size([602, 1])\n",
      "target_tensor: torch.Size([16, 1])\n",
      "0m 53s (- 0m 7s) (88 88%) 2.9642\n",
      "iter: 89\n",
      "input_tensor: torch.Size([1032, 1])\n",
      "target_tensor: torch.Size([26, 1])\n",
      "0m 53s (- 0m 6s) (89 89%) 2.8825\n",
      "iter: 90\n",
      "input_tensor: torch.Size([1158, 1])\n",
      "target_tensor: torch.Size([29, 1])\n",
      "0m 54s (- 0m 6s) (90 90%) 2.9901\n",
      "iter: 91\n",
      "input_tensor: torch.Size([1114, 1])\n",
      "target_tensor: torch.Size([28, 1])\n",
      "0m 55s (- 0m 5s) (91 91%) 3.1460\n",
      "iter: 92\n",
      "input_tensor: torch.Size([479, 1])\n",
      "target_tensor: torch.Size([13, 1])\n",
      "0m 55s (- 0m 4s) (92 92%) 2.7548\n",
      "iter: 93\n",
      "input_tensor: torch.Size([1168, 1])\n",
      "target_tensor: torch.Size([29, 1])\n",
      "0m 56s (- 0m 4s) (93 93%) 2.6925\n",
      "iter: 94\n",
      "input_tensor: torch.Size([1206, 1])\n",
      "target_tensor: torch.Size([30, 1])\n",
      "0m 57s (- 0m 3s) (94 94%) 2.9352\n",
      "iter: 95\n",
      "input_tensor: torch.Size([649, 1])\n",
      "target_tensor: torch.Size([17, 1])\n",
      "0m 57s (- 0m 3s) (95 95%) 2.8329\n",
      "iter: 96\n",
      "input_tensor: torch.Size([701, 1])\n",
      "target_tensor: torch.Size([18, 1])\n",
      "0m 58s (- 0m 2s) (96 96%) 2.5596\n",
      "iter: 97\n",
      "input_tensor: torch.Size([903, 1])\n",
      "target_tensor: torch.Size([23, 1])\n",
      "0m 58s (- 0m 1s) (97 97%) 2.8098\n",
      "iter: 98\n",
      "input_tensor: torch.Size([523, 1])\n",
      "target_tensor: torch.Size([14, 1])\n",
      "0m 59s (- 0m 1s) (98 98%) 2.9959\n",
      "iter: 99\n",
      "input_tensor: torch.Size([1081, 1])\n",
      "target_tensor: torch.Size([27, 1])\n",
      "0m 59s (- 0m 0s) (99 99%) 2.9128\n",
      "iter: 100\n",
      "input_tensor: torch.Size([823, 1])\n",
      "target_tensor: torch.Size([21, 1])\n",
      "1m 0s (- 0m 0s) (100 100%) 2.5394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdyUlEQVR4nO3deXiU1cH38e/JZN+IEJYAAQyLAcJSTVEUUFEsGgVs34pXF3y64GMXaqV9S9++KAhan0dtVVoVt6qtra1Pq6DsboBgAQFZEpawBCSSQAAJhCwMyXn+mBExJGECSc7M5Pe5rrmuTO7DzI9zJb+5c5977jHWWkREJPRFuA4gIiJNQ4UuIhImVOgiImFChS4iEiZU6CIiYSLS1ROnpqbaHj16uHp6EZGQtG7dukPW2vZ1bXNW6D169GDt2rWunl5EJCQZY/bWt02HXEREwoQKXUQkTKjQRUTChApdRCRMqNBFRMKECl1EJEyo0EVEwkTIFXrhZ+Xc/1Ye3uoa11FERIJKyBX61qLjvLhyDy+t3OM6iohIUAm5Qh/VryPXZXbgsXfyKSqtcB1HRCRohFyhA0wf05/qGsvMeVtcRxERCRohWejpbeOZNLIXCzYXsyy/xHUcEZGgEJKFDjBxRAYZqQlMm5tLpbfadRwREedCttBjIj3MGJvFnsPlPLNst+s4IiLOhWyhAwzrncotgzrz5NKd7D18wnUcERGnQrrQAabm9CXaE8F9c/Ow1rqOIyLiTMgXesfkWO4Z1Ydl+SUszit2HUdExJmQL3SAO4Z2J7NTEjPe2sKJqlOu44iIOBEWhR7pieDBW7PYX1rJrPd2uI4jIuJEWBQ6wGXd2zI+O50XPigg/8Bx13FERFpc2BQ6wJQbM0mMjWTqnFwtkIpIqxNWhd42IZopozNZU3CENz7+1HUcEZEWFVaFDjA+O53B6Sn8dsFWSiu8ruOIiLSYsCv0iAjDA+OyOHLiJL9bst11HBGRFhN2hQ6Q1aUNE4b24C+r9rK5sNR1HBGRFhGWhQ4w+YY+pCbGMHXOZqprtEAqIuEvbAs9OTaKqTl92VhYyqtrPnEdR0Sk2YVtoQOMGdSZoRnteHjRNg6VVbmOIyLSrMK60I0xzBzXnwpvNQ8t2OY6johIswrrQgfo1SGJicMz+Nf6QtYUHHEdR0Sk2YR9oQNMGtmbLilx3DsnF291jes4IiLNolUUely0h+lj+rP9wHFeWrnHdRwRkWbRKgodYFS/jlyX2YHH3smnqLTCdRwRkSbXagodYPqY/lTXWGbO2+I6iohIk2tVhZ7eNp5JI3uxYHMxy/JLXMcREWlSrarQASaOyCAjNYFpc3Op9Fa7jiMi0mRaXaHHRHqYMTaLPYfLeWbZbtdxRESaTKsrdIBhvVO5ZVBnnly6k72HT7iOIyLSJFploQNMzelLtCeC++bm6dONRCQstNpC75gcyz2j+rAsv4TFecWu44iIXLBWW+gAdwztTmanJO5/awsnqk65jiMickFadaFHeiJ48NYsikormfXeDtdxREQuSKsudIDLurdlfHY6L3xQQP6B467jiIict1Zf6ABTbswkMTaSqXNytUAqIiFLhQ60TYhmyuhM1hQc4Y2PP3UdR0TkvKjQ/cZnpzM4PYXfLthKabnXdRwRkUZToftFRBgeGJfFkRMneXTJdtdxREQaTYV+hqwubZgwtAevrN7LpsKjruOIiDSKCr2WyTf0ITUxhnvn5FJdowVSEQkdKvRakmOjmJrTl42Fpby65hPXcUREAqZCr8OYQZ0ZmtGOhxdt41BZles4IiIBUaHXwRjDzHH9qfBW89CCba7jiIgERIVej14dkpg4PIN/rS9k9e7DruOIiJyTCr0Bk0b2pktKHPfOzcVbXeM6johIg1ToDYiL9jB9TH/yD5Tx0so9ruOIiDRIhX4Oo/p15LrMDjz2Tj5FpRWu44iI1EuFHoDpY/pTXWOZOW+L6ygiIvVSoQcgvW08k0b2YsHmYpZuP+g6johInVToAZo4IoOM1ASmvZlHpbfadRwRkbOo0AMUE+lhxtgs9h4uZ/ayXa7jiIicRYXeCMN6p3LLoM48tXQXew+fcB1HRORLVOiNNDWnL9GeCO6bm6dPNxKRoKJCb6SOybHcM6oPy/JLWJxX7DqOiMhpKvTzcMfQ7mR2SuL+t7ZwouqU6zgiIkAAhW6MiTXGrDHGbDTG5Blj7q9jzLeNMZv8tw+NMYOaJ25wiPRE8OCtWRSVVjLr3R2u44iIAIHtoVcBI621g4DBwGhjzBW1xhQAV1trBwIzgWebNmbwuax7W8Znp/PCigLyDxx3HUdE5NyFbn3K/Hej/Ddba8yH1trP/HdXAV2bNGWQmnJjJomxkUydk6sFUhFxLqBj6MYYjzFmA3AQeNtau7qB4T8AFtbzOHcaY9YaY9aWlJQ0Pm2QaZsQzZTRmawpOMIbH3/qOo6ItHIBFbq1ttpaOxjfnvcQY0xWXeOMMdfiK/Qp9TzOs9babGttdvv27c83c1AZn53O4PQUfrtgK6XlXtdxRKQVa9RZLtbao8BSYHTtbcaYgcDzwFhrbav5RIiICMMD47I4cuIkjy7Z7jqOiLRigZzl0t4Yk+L/Og64HthWa0w34HXgu9ba/OYIGsyyurRhwtAevLJ6L5sKj7qOIyKtVCB76GnA+8aYTcBH+I6hzzPG3GWMucs/5j6gHfCUMWaDMWZtM+UNWpNv6ENqYgxT5+RSXaMFUhFpecbV2RnZ2dl27drw6v25Gz7l7r9v4IFxWXzniu6u44hIGDLGrLPWZte1Te8UbUJjBnVmaEY7Hl60jUNlVa7jiEgro0JvQsYYZo7rT4W3mocWbDv3PxARaUIq9CbWq0MSE4dn8K/1haze3WpO9hGRIKBCbwaTRvamS0oc987NxVtd4zqOiLQSKvRmEBftYfqY/uQfKOPFlQWu44hIK6FCbyaj+nXk+r4dePydHRSVVriOIyKtgAq9GU27pT/VNZaZ87a4jiIirYAKvRmlt41n0sheLNhczNLtB13HEZEwp0JvZhNHZJCRmsC0N/Oo9Fa7jiMiYUyF3sxiIj3MGJvF3sPlzF62y3UcEQljKvQWMKx3KrcM6sxTS3ex59AJ13FEJEyp0FvI1Jy+RHsimPZmnj7dSESahQq9hXRMjuWeUX1Yll/C4rxi13FEJAyp0FvQHUO7k9kpifvf2sKJqlOu44hImFGht6BITwQP3ppFUWkls97d4TqOiIQZFXoLu6x7W8Znp/PCigK2Fx93HUdEwogK3YEpN2aSGBvJvXNytUAqIk1Ghe5A24RopozOZM2eI7y+/lPXcUQkTKjQHRmfnc7g9BQeWriV0nKv6zgiEgZU6I5ERBgeGJfFkRMneXTJdtdxRCQMqNAdyurShglDe/DK6r1sKjzqOo6IhDgVumOTb+hDamIMU+fkUl2jBVIROX8qdMeSY6OYmtOXTYWl/G3NJ67jiEgIU6EHgTGDOjM0ox2PLNrGobIq13FEJESp0IOAMYaZ4/pT4a3moQXbXMcRkRClQg8SvTokMXF4Bv9aX8jq3YddxxGREKRCDyKTRvamS0oc987NxVtd4zqOiIQYFXoQiYv2MH1Mf/IPlPHiygLXcUQkxKjQg8yofh25vm8HHn9nB/uPVriOIyIhRIUehKbd0p/qGsvMeVtcRxGREKJCD0LpbeOZNLIXC3OLWbr9oOs4IhIiVOhBauKIDDJSE5j2Zh6V3mrXcUQkBKjQg1RMpIcZY7PYe7ic2ct2uY4jIiFAhR7EhvVO5ZZBnXlq6S72HDrhOo6IBDkVepCbmtOXaE8E972Zp083EpEGqdCDXMfkWO4Z1Yfl+SUsyi12HUdEgpgKPQTcMbQ7mZ2SmDFvCyeqTrmOIyJBSoUeAiI9ETx4axZFpZXMeneH6zgiEqRU6CHisu5tGZ+dzgsrCthefNx1HBEJQir0EDLlxkwSYyO5d06uFkhF5Cwq9BDSNiGaX4/OZM2eI7y+/lPXcUQkyKjQQ8xt2el8pVsKv12wldJyr+s4IhJEVOghJiLCMHNsFp+Vn+TRJdtdxxGRIKJCD0FZXdowYWgPXlm9l02FR13HEZEgoUIPUZNv6ENqYgxT5+RSXaMFUhFRoYes5Ngopub0ZVNhKX9b84nrOCISBFToIWzMoM5c2bMdDy/aRsnxKtdxRMQxFXoIM8YwY2wWld5qHlq41XUcEXFMhR7ienVIZOLwDF5f/ymrdx92HUdEHFKhh4FJI3vTJSWOe+fm4q2ucR1HRBxRoYeBuGgP08f0J/9AGS+uLHAdR0QcUaGHiVH9OnJ93w48/s4O9h+tcB1HRBxQoYeRabf0p7rGMnPeFtdRRMQBFXoYSW8bz6SRvViYW8z72w+6jiMiLUyFHmYmjsggIzWB6W/mUemtdh1HRFqQCj3MxER6mDE2i72Hy5m9bJfrOCLSglToYWhY71RuGdSZp5buYs+hE67jiEgLUaGHqak5fYn2RHDfm3n6dCORVkKFHqY6Jsdyz6g+LM8vYVFuses4ItICVOhh7I6h3cnslMT9b21h/SefaU9dJMyp0MNYpCeC3359AKUVXr7+1IcM++/3eXD+Fj5WuYuEJePqFzs7O9uuXbvWyXO3NqUVXt7ZcoD5m4v4YEcJ3mpLl5Q4cgamcdOANAZ1bYMxxnVMEQmAMWadtTa7zm0q9NaltMLL21sOMH/TflbsPIS32tL1ojhyBvjKfaDKXSSoqdClTqXlXpZsKWb+5iJW7DjEqZovyj1nYBoDuqjcRYLNBRW6MSYWWA7EAJHAP62102qNMcATwE1AOfAf1tr1DT2uCj24HC0/yZItB5i/qYiVO33lnt42jpsGpHHzgM5kdUlWuYsEgQstdAMkWGvLjDFRwArgbmvtqjPG3ARMwlfolwNPWGsvb+hxVejB62j5SZbk+Y65f17u3drG+8p9YBr9O6vcRVxpqNAjz/WPra/xy/x3o/y32q8CY4E/+8euMsakGGPSrLVFF5BbHEmJj+a2r6Zz21fT+ezESf9hmWKe+2A3s5ftolvbeHIGppEzQOUuEkzOWegAxhgPsA7oBTxprV1da0gXYN8Z9wv93/tSoRtj7gTuBOjWrdt5RpaWdFFCNOO/2o3xX+12utznbSri2eW7eXrpLrq3iz+9oKpyF3GrUYuixpgU4A1gkrU294zvzwcestau8N9/F/iVtXZdfY+lQy6h7ciJkyzJ8y2ofrjrMNU1lh7t4k+fCtkvTeUu0hwu6JDLmay1R40xS4HRQO4ZmwqB9DPudwX2NzKnhJC2CdHcPqQbtw/pxpETJ1mcV8z8TUXMXrabJ9/fxcWpCaf33PumJancRVpAIIui7QGvv8zjgCXAf1tr550xJgf4KV8sis6y1g5p6HG1hx6eDpdVsTjvAPM37+ffuw5TYyEjNeH0nntmJ5W7yIW40LNcBgIvAx58lwp4zVo7wxhzF4C1drb/TJg/4ttzLwe+Z61tsK1V6OHvUFnV6T33Vbv95d4+4fR57pd0VLmLNJbeWCTOHSqrYlGur9xXF/jKvefpcu9Mn46JKneRAKjQJaiUHK9iUV4xC2qX+8DO3DwwjT4dk1xHFAlaKnQJWgePV7I413e2zOqCI1gLvTsknn4TU2+Vu8iXqNAlJHxe7vM2FbFmzxfl/vmbmFTuIip0CUEHj1WyKM9X7h/5y71Px0RyBnQmZ2AnenVQuUvrpEKXkHbgWOXpBdWP9vrK/ZKOSadPhezVIdF1RJEWo0KXsHHgWCULNxcxf3MRa/d+hrWQ2SnJ9yamgWn0bK9yl/CmQpewVFxaycLcIuZv8pU7+Mr9Zv+ee4bKXcKQCl3CXlFpBQs3+86WWecv975pyeQM6KRyl7CiQpdWpai0ggWbi1lwRrn3S0s+fcz94tQExwlFzp8KXVqt/UcrWLC5iAWbi1j/yVHgi3LPGZBGD5W7hBgVugjw6dGK0wuqH/vLvX/nL8q9ezuVuwQ/FbpILZ+X+7xNRWzY5yv3rC7JvvPcB6TRrV2844QidVOhizSg8LNyFm4uZt7mIjb6yz1nQBq/Gn2J9tol6KjQRQK070g5/7N2H899UMCpmhomDO3BpJG9SImPdh1NBFChizTagWOVPPZ2Pq+t3UdiTCSTRvZmwpXdiYn0uI4mrVxDhR7R0mFEQkHH5Fj+6xsDWXj3CC7tfhEPLtjK9b9fxpsb9+NqJ0jkXFToIg24pFMSL31vCH/5wRASY6L42asfM+6pD1lTcMR1NJGzqNBFAjC8d3vmTRrGo98cxIHSSm575t/c+ee17C4pcx1N5DQdQxdppIqT1bywYjdPL91F1akavn15N352XW/aJca4jiatgBZFRZpByfEqnng3n1fX7CM+ysOPru3J96+6mNgoLZxK89GiqEgzaJ8UwwPjBrD45yO4PKMdDy/azshHl/L6+kJqarRwKi1PhS5ygXp1SOT5O7J5deIVtEuMYfJrGxnz5Ao+3HXIdTRpZVToIk1kaM92zP3JVTxx+2A+O+HlW8+t5gcvfcSOA8ddR5NWQoUu0oQiIgxjB3fh3V9cza9vzGRNwRG+9vhyfvPGZg4er3QdT8KcFkVFmtGREyeZ9e4OXlm1l+jICO66uic/HH4x8dGRrqNJiNKiqIgjbROimT6mP29PvpoRvdvz+7fzufbRpby2dh/VWjiVJqZCF2kBF6cmMPu7l/HPu4aS1iaOX/1zEzmzPmB5fonraBJGVOgiLSi7R1ve+PGVPPmtSzlx8hQT/rSGCX9aw9aiY66jSRhQoYu0MGMMOQPTeGfy1UzN6cvGfUe5adYH/OqfGzlwTAuncv60KCriWGm5lz++v4OXP9yLJ8IwcfjF3Hl1TxJjtHAqZ9OiqEgQaxMfxf/P6cc7k6/mur4dmPXeTq55ZCl/W/0Jp6prXMeTEKJCFwkS3drF88dvXcobP76Si1Pj+c0bmxn9xAe8t+2ArsEuAVGhiwSZr3S7iNf+cyizv3MZ1TWW77+0lm8/v5rcT0tdR5Mgp0IXCULGGEZndWLJPSO4f0x/thYd4+Y/rGDyPzaw/2iF63gSpLQoKhICjlV6eXrpLl5YUYABvj/sYn50TU+SY6NcR5MWpkVRkRCXHBvFlNGZvPeLq7lpQBpPL93FNY8s5c//3oNXC6fip0IXCSFdL4rnsfGDeeunw+jTMZH75ubxtceWszivWAunokIXCUUDurbh1YlX8MId2RgD//mXdYx/ZhUb9h11HU0cUqGLhChjDNf17cjin4/gwVuz2H2ojHFPrmTSqx+z70i563jigBZFRcJEWdUpnlm2i+c+2E1NDdxxZXd+em1v2sRr4TScaFFUpBVIjInkFzdcwtJfXsvYwZ15fkUBIx55nxdWFHDylBZOWwMVukiY6dQmlke+OYj5k4YzsGsbZs7bwqjHljF/U5EWTsOcCl0kTPXrnMxffnA5L39/CLGRHn7yt/V84+kPWbf3iOto0kxU6CJh7uo+7Vlw93Ae/sZACj+r4BtP/5sfvbKOPYdOuI4mTUzX5xRpBTwRhtu+ms7Ng9J4bnkBzyzfxTtbD/CdK7rzs5G9uSgh2nVEaQI6y0WkFTp4vJLH3t7BPz76hISYSH56bS/uuLIHsVEe19HkHHSWi4h8SYekWB76+gAW/XwE2d0v4qGF27jud8uYu+FTavTh1SFLhS7SivXpmMSL3xvCX394OW3iorj77xsY99RKVu0+7DqanAcVuohwVa9U5k0axu9vG0TJ8Spuf3YVP3x5LTsPlrmOJo2gQhcRACIiDF+/tCvv//Ia/u/XLmHV7sN87fHl3Dsnl0NlVa7jSQC0KCoidTpUVsWsd3fw19WfEBfl4UfX9OT7V11MXLQWTl3SoqiINFpqYgwzxmax5J4RDO3ZjkcWb2fk75byz3WFWjgNUip0EWlQz/aJPDchm3/ceQUdkmL45f9s5OY/rGDlzkOuo0ktKnQRCcjlGe1448dX8cTtgymt8PLt51fzHy+uYXvxcdfRxE/H0EWk0Sq91fz533v4w3s7Kas6RYekGNrERZESF01yXBRtzrilxH/xdXKt70V5tE/ZWA0dQ9db/0Wk0WKjPNw5oiffvCydv6zay74j5ZRWeCmt8FL4WTlb9ns5WuGl/GR1g48TH+35UvmfdYuvf1ukXgzOokIXkfN2UUI0P7uud73bT56q4Vil93TZl5af8fUZt6PlXo5VeNl7+IsXhgpvwy8GiTGRZ+z1R5ISF/2lF4HkuChS6nghSI6LwhNhmnoqgoIKXUSaTXRkBKmJMaQmxjT631adqqa0wlf0tcv/zPufb99VUnb6e1Xn+ECPpNjIs4o+JT7q7MNFn79I+G9JsZFEBPGLgQpdRIJSTKSHDkkeOiTFNvrfVnqrOVbhO+zT0F8Gn992HCw7Pe5kdf0vBsZAUkwkKfHRZ+31pzRweKhNfBRJMZEY07wvBip0EQk7sVEeYqM8dEhu3IuBtZZKb02tvwhOnvXXQOkZLxb7SytOf99bXf9JJhGG04eBvnNFd344PONC/5tnUaGLiPgZY4iL9hAX7aFTm8a/GJSfrK77r4BafyGczyGoQKjQRUSagDGGhJhIEmIi6ZwS5ySDzvsREQkTKnQRkTBxzkI3xqQbY943xmw1xuQZY+6uY0wbY8xbxpiN/jHfa564IiJSn0COoZ8CfmGtXW+MSQLWGWPettZuOWPMT4At1tpbjDHtge3GmL9aa082R2gRETnbOffQrbVF1tr1/q+PA1uBLrWHAUnGd5JlInAE3wuBiIi0kEYdQzfG9AC+AqyutemPQF9gP7AZuNtae9bZ+caYO40xa40xa0tKSs4rsIiI1C3gQjfGJAL/An5urT1Wa/PXgA1AZ2Aw8EdjTHLtx7DWPmutzbbWZrdv3/4CYouISG0BFboxJgpfmf/VWvt6HUO+B7xufXYCBUBm08UUEZFzOef10P3HxV8Gjlhrf17PmKeBA9ba6caYjsB6YJC1tt6PNDHGlAB7zzN3KhCMH5cSrLkgeLMpV+MoV+OEY67u1to6D3EEUujDgA/wHRv//Lj4b4BuANba2caYzsBLQBpggP+y1r5ynmHPyRiztr4LvLsUrLkgeLMpV+MoV+O0tlznPG3RWrsCX0k3NGY/cENThRIRkcbTO0VFRMJEqBb6s64D1CNYc0HwZlOuxlGuxmlVuZx9SLSIiDStUN1DFxGRWlToIiJhIqgL3Rgz2hiz3Riz0xjz6zq2G2PMLP/2TcaYS4Mk1zXGmFJjzAb/7b4WyvUnY8xBY0xuPdtdzde5crX4fAV4FdEWn68Ac7mYr1hjzJozrqh6fx1jXMxXILmc/D76n9tjjPnYGDOvjm1NP1/W2qC8AR5gF5ABRAMbgX61xtwELMR3WuUVwOogyXUNMM/BnI0ALgVy69ne4vMVYK4Wny9875m41P91EpAfJD9fgeRyMV8GSPR/HYXvek5XBMF8BZLLye+j/7knA3+r6/mbY76CeQ99CLDTWrvb+i7D+3dgbK0xY4E/W59VQIoxJi0IcjlhrV2O70qX9XExX4HkanE2sKuItvh8BZirxfnnoMx/N8p/q31GhYv5CiSXE8aYrkAO8Hw9Q5p8voK50LsA+864X8jZP9iBjHGRC2Co/8/AhcaY/s2cKVAu5itQzubL1H8VUafz1UAucDBf/sMHG4CDwNvW2qCYrwBygZufr8eBX/HFO+xra/L5CuZCr+vdqbVfeQMZ09QCec71+K63MAj4AzCnmTMFysV8BcLZfJmGryLqbL7OkcvJfFlrq621g4GuwBBjTFatIU7mK4BcLT5fxpibgYPW2nUNDavjexc0X8Fc6IVA+hn3u+K73npjx7R4Lmvtsc//DLTWLgCijDGpzZwrEC7m65xczZc591VEnczXuXK5/vmy1h4FlgKja21y+vNVXy5H83UVMMYYswffYdmRxpja17dq8vkK5kL/COhtjLnYGBMN3A68WWvMm8AE/2rxFUCptbbIdS5jTCdjjPF/PQTfPB9u5lyBcDFf5+RivvzP9wKw1Vr7+3qGtfh8BZLL0Xy1N8ak+L+OA64HttUa5mK+zpnLxXxZa/+ftbartbYHvo54z1r7nVrDmny+AvlMUSestaeMMT8FFuM7s+RP1to8Y8xd/u2zgQX4Vop3AuX4rsseDLn+D/AjY8wpoAK43fqXtZuTMeZVfCv6qcaYQmAavkUiZ/MVYC4X83UV8F1gs//4K9S6iihu5iuQXC7mKw142RjjwVeIr1lr57n+fQwwl5Pfx7o093zprf8iImEimA+5iIhII6jQRUTChApdRCRMqNBFRMKECl1EJEyo0EVEwoQKXUQkTPwvzfHef3EX12QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#n_iters是训练集数据的个数，可以小于，但不能大于\n",
    "#epochs是可调参数，代表训练次数\n",
    "trainIters(encoder1, attn_decoder1, n_iters=len(x_train),epochs=1,print_every=1,plot_every=20,save_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> torch.Size([911, 1])\n",
      "= torch.Size([23, 1])\n",
      "< <S> F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5 F5 F5 A5 F5 F5 A5 F5 A5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#x_train和y_train是测试数据。\n",
    "#n代表测试多少个测试数据\n",
    "res=evaluateRandomly(encoder1, attn_decoder1,x_test,y_test,n=2)\n",
    "# res,att=evaluate(encoder1,attn_decoder1,x_test)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> G4 <G5> C5 G5 <E5> D5-- B6 <C5> C5#--- G5 C5# A5# <<F4#>> <<F5>> B5 A5# <A4> B5 A6# <G6> D5- C4. <B5> E5 F5 <E> "
     ]
    }
   ],
   "source": [
    "for x in y_test:\n",
    "    note=num_dic[x.item()]\n",
    "    print(note,end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-59285e48db97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "for x in list(y_test):\n",
    "    y.append(num_dic[x.item()])\n",
    "\n",
    "for x in range(len(res)):\n",
    "    if res[x] in y :\n",
    "        print(x,res[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', 'C5', 'F5', 'C5', 'B5', 'F5', 'G5', 'G4', 'E5', 'B5', 'B5', 'B5', 'E5', 'D5', 'D5', 'E5', 'F6', 'A5', 'F6', 'A5', 'D5', 'D5', 'D5', 'D4', 'B4', 'F5', 'D4', '<E>']\n",
      "<S> C5 F5 C5 B5 F5 G5 G4 A5 E5 B5 A6 A4 C5 D5 E5 E4 F6 F6 C4 D5 D5 C4 F5 <E> \n",
      "\n",
      "==============================\n",
      "['<S>', 'F5', 'D4', 'B5', 'F5', 'B4', 'E5', 'E5', 'C4', 'C4', 'C4', 'A5', 'D4', 'D4', 'D4', 'D4', 'D4', 'B4', 'G5', 'D4', 'B4', 'G5', 'D4', '<E>']\n",
      "<S> F5 D4 B5 F5 B4 G6 E5 A5 C4 G6 C4 A5 D4 D5 D4 E5 E6 B5 C6 A4 C4 F5 A6 B6 A6 E6 <E> \n",
      "\n",
      "==============================\n",
      "['<S>', 'F5', 'C5', 'C5', 'C4', 'A5', 'F4', 'A5', 'E5', 'D5', 'A5', 'E5', 'E5', 'E5', 'B5', 'E5', 'E5', 'G5', 'E5', 'G5', '<E>']\n",
      "<S> F5 C5 C5 C4 A5 F4 A5 E5 D5 A5 E5 A4 B5 E5 G5 F5 <E> \n",
      "\n",
      "==============================\n",
      "['<S>', 'A5', 'A6', 'B5', 'A5', 'A5', 'C5', 'F5', 'E5', 'F4', 'A5', 'A5', 'F4', 'D5', 'D5', 'G5', '<E>']\n",
      "<S> A5 A6 B5 A5 A5 C5 F5 G4 E5 F4 A5 B6 E4 D5 A5 <E> \n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "encoder1=torch.load('model/ecnoder80-100.pkl')\n",
    "attn_decoder1=torch.load('model/attn_decoder80-100.pkl')\n",
    "\n",
    "# res=evaluateRandomly(encoder1, attn_decoder1,x_test,y_test,n=1)\n",
    "for x in range(len(x_test)):\n",
    "    res,att=evaluate(encoder1,attn_decoder1,x_test[x])\n",
    "    print(res)\n",
    "    for x in y_test[x]:\n",
    "        note=num_dic[x.item()]\n",
    "        print(note,end=' ')\n",
    "    print('\\n')\n",
    "    print('==='*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取模型继续训练\n",
    "encoder1=torch.load('model/ecnoder80-100.pkl')\n",
    "attn_decoder1=torch.load('model/attn_decoder80-100.pkl')\n",
    "#注意继续训练时，保存模型的名称\n",
    "trainIters(encoder1, attn_decoder1, n_iters=len(x_train),epochs=100,print_every=1,plot_every=20,save_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> C5 F5 C5 B5 F5 G5 G4 A5 E5 B5 A6 A4 C5 D5 E5 E4 F6 F6 C4 D5 D5 C4 F5 <E> "
     ]
    }
   ],
   "source": [
    "\n",
    "for x in y_test[0]:\n",
    "    note=num_dic[x.item()]\n",
    "    print(note,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> torch.Size([1219, 1])\n",
      "= torch.Size([29, 1])\n",
      "< <S> D5- B5 C6 <<C5>> E5 D5 A5 G5 G5 C4# A5 E5 E5 E5 E5 <E>\n",
      "\n",
      "0 74 35 4 43 14 8 29 23 23 7 29 14 14 14 14 1 \n",
      "\n",
      "tensor([0]) tensor([8]) tensor([239]) tensor([205]) tensor([9]) tensor([9]) tensor([31]) tensor([35]) tensor([10]) tensor([201]) tensor([14]) tensor([18]) tensor([176]) tensor([5]) tensor([14]) tensor([96]) tensor([78]) tensor([23]) tensor([14]) tensor([19]) tensor([132]) tensor([241]) tensor([14]) tensor([248]) tensor([30]) tensor([108]) tensor([130]) tensor([8]) tensor([1]) "
     ]
    }
   ],
   "source": [
    "# note_dic,num_dic\n",
    "encoder1=torch.load('model/ecnoder70-100.pkl')\n",
    "attn_decoder1=torch.load('model/attn_decoder70-100.pkl')\n",
    "\n",
    "res=evaluateRandomly(encoder1, attn_decoder1,x_train,y_train,n=1)\n",
    "for x in res:\n",
    "    print(note_dic[x],end=' ')\n",
    "print('\\n')\n",
    "for x in y_train[0]:\n",
    "    print(x,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.load('Loss/loss100.npy')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('wavDatasets/2020-05-26_17_15_16_0.wav')\n",
    "librosa.display.waveplot(y, sr=sr)\n",
    "plt.title('Monophonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "unknown format: 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-ac72d4c22daf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wavDatasets/2020-05-26_17_15_16_0.wav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 读取格式信息\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# (nchannels, sampwidth, framerate, nframes, comptype, compname)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\python36\\lib\\wave.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWave_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mWave_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\python36\\lib\\wave.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# else, assume it is an open file object already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitfp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\python36\\lib\\wave.py\u001b[0m in \u001b[0;36minitfp\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mchunkname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchunkname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb'fmt '\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_fmt_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fmt_chunk_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mchunkname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb'data'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\python36\\lib\\wave.py\u001b[0m in \u001b[0;36m_read_fmt_chunk\u001b[1;34m(self, chunk)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msampwidth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unknown format: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwFormatTag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_framesize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nchannels\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comptype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NONE'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: unknown format: 3"
     ]
    }
   ],
   "source": [
    "\n",
    "import wave\n",
    "f = wave.open(\"wavDatasets/2020-05-26_17_15_16_0.wav\", \"rb\")\n",
    "# 读取格式信息\n",
    "# (nchannels, sampwidth, framerate, nframes, comptype, compname)\n",
    "params = f.getparams()\n",
    "nchannels, sampwidth, framerate, nframes = params[:4]\n",
    "# 读取波形数据\n",
    "str_data = f.readframes(nframes)\n",
    "f.close()\n",
    "#将波形数据转换为数组\n",
    "wave_data = np.fromstring(str_data, dtype=np.short)\n",
    "wave_data.shape = -1, 2\n",
    "wave_data = wave_data.T\n",
    "time = np.arange(0, nframes) * (1.0 / framerate)\n",
    "# 绘制波形\n",
    "\n",
    "plt.plot(time, wave_data[0])\n",
    "\n",
    "plt.plot(time, wave_data[1], c=\"g\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
